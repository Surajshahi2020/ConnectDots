import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import re
import time
import os
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def techpana_to_json():
    base_url = "https://techpana.com/"
    
    # Rotating User-Agents
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
    ]
    
    # Enhanced headers
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
    }
    
    # Comprehensive Nepali keywords for threat detection
    important_keywords = {
        # Cybersecurity Threats
        "‡§∏‡§æ‡§á‡§¨‡§∞", "‡§π‡•ç‡§Ø‡§æ‡§ï", "‡§π‡•ç‡§Ø‡§æ‡§ï‡§ø‡§ô", "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§Ö‡§™‡§∞‡§æ‡§ß", "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§π‡§Æ‡§≤‡§æ", "‡§°‡§æ‡§ü‡§æ", "‡§°‡§æ‡§ü‡§æ ‡§â‡§≤‡•ç‡§≤‡§Ç‡§ò‡§®",
        "‡§´‡§ø‡§∏‡§ø‡§ô", "‡§Æ‡§æ‡§≤‡§µ‡•á‡§Ø‡§∞", "‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§∏‡§Æ‡§µ‡•á‡§Ø‡§∞", "‡§≠‡§æ‡§á‡§∞‡§∏", "‡§ü‡•ç‡§∞‡•ã‡§ú‡§®", "‡§∏‡•ç‡§™‡§æ‡§á‡§µ‡•á‡§Ø‡§∞",
        "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§∏‡•Ç‡§ö‡§®‡§æ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§Ö‡§®‡§≤‡§æ‡§á‡§® ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ",
        "‡§™‡§æ‡§∏‡§µ‡§∞‡•ç‡§°", "‡§è‡§®‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∏‡§®", "‡§´‡§æ‡§Ø‡§∞‡§µ‡§æ‡§≤", "‡§è‡§®‡•ç‡§ü‡§ø‡§≠‡§æ‡§á‡§∞‡§∏",
        
        # Data Privacy & Breaches
        "‡§°‡§æ‡§ü‡§æ ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§£", "‡§ó‡•ã‡§™‡§®‡•Ä‡§Ø‡§§‡§æ", "‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§°‡§æ‡§ü‡§æ", "‡§®‡§ø‡§ú‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä", "‡§°‡§æ‡§ü‡§æ ‡§≤‡§ø‡§ï",
        "‡§ó‡•ã‡§™‡§®‡•Ä‡§Ø‡§§‡§æ ‡§â‡§≤‡•ç‡§≤‡§Ç‡§ò‡§®", "‡§°‡§æ‡§ü‡§æ ‡§ö‡•ã‡§∞‡•Ä", "‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≤‡§ø‡§ï", "‡§™‡•ç‡§∞‡§æ‡§á‡§≠‡•á‡§∏‡•Ä",
        
        # Financial Cyber Crimes
        "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§¨‡•à‡§Ç‡§ï‡§ø‡§ô", "‡§á‡§®‡•ç‡§ü‡§∞‡§®‡•á‡§ü ‡§¨‡•à‡§Ç‡§ï‡§ø‡§ô", "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§¨‡•à‡§Ç‡§ï‡§ø‡§ô", "‡§ï‡•ç‡§∞‡•á‡§°‡§ø‡§ü ‡§ï‡§æ‡§∞‡•ç‡§°", "‡§°‡•á‡§¨‡§ø‡§ü ‡§ï‡§æ‡§∞‡•ç‡§°",
        "‡§ï‡•ç‡§∞‡•á‡§°‡§ø‡§ü ‡§ï‡§æ‡§∞‡•ç‡§° ‡§ß‡•ã‡§ï‡§æ", "‡§¨‡•à‡§Ç‡§ï‡§ø‡§ô ‡§ß‡•ã‡§ï‡§æ", "‡§Ö‡§®‡§≤‡§æ‡§á‡§® ‡§ß‡•ã‡§ï‡§æ", "‡§à-‡§ï‡•â‡§Æ‡§∞‡•ç‡§∏ ‡§ß‡•ã‡§ï‡§æ",
        "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§™‡•á‡§Æ‡•á‡§®‡•ç‡§ü", "‡§à-‡§™‡•á‡§Æ‡•á‡§®‡•ç‡§ü", "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§™‡•á‡§Æ‡•á‡§®‡•ç‡§ü", "‡§µ‡§æ‡§≤‡•ç‡§ü", "‡§∏‡§æ‡§®‡§ø‡§Æ‡§æ ‡§¨‡•à‡§Ç‡§ï", "‡§á‡§®‡•ç‡§ü‡§∞‡§®‡•á‡§ü ‡§¨‡•à‡§Ç‡§ï‡§ø‡§ô",
        
        # Social Media & Online Crimes
        "‡§∏‡•ã‡§∂‡§≤ ‡§Æ‡§ø‡§°‡§ø‡§Ø‡§æ", "‡§´‡•á‡§∏‡§¨‡•Å‡§ï", "‡§ü‡•ç‡§µ‡§ø‡§ü‡§∞", "‡§á‡§®‡•ç‡§∏‡•ç‡§ü‡§æ‡§ó‡•ç‡§∞‡§æ‡§Æ", "‡§ü‡§ø‡§ï‡§ü‡§ï", "‡§Ø‡•Å‡§ü‡•ç‡§Ø‡•Å‡§¨",
        "‡§∏‡•ã‡§∂‡§≤ ‡§Æ‡§ø‡§°‡§ø‡§Ø‡§æ ‡§Ö‡§™‡§∞‡§æ‡§ß", "‡§Ö‡§®‡§≤‡§æ‡§á‡§® ‡§â‡§§‡•ç‡§™‡•Ä‡§°‡§®", "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§¨‡•Å‡§≤‡§ø‡§ô", "‡§Ö‡§®‡§≤‡§æ‡§á‡§® ‡§ß‡§Æ‡•ç‡§ï‡•Ä",
        "‡§´‡•á‡§ï ‡§Ö‡§ï‡§æ‡§â‡§®‡•ç‡§ü", "‡§®‡§ï‡§≤‡•Ä ‡§™‡•ç‡§∞‡•ã‡§´‡§æ‡§á‡§≤", "‡§á‡§®‡•ç‡§ü‡§∞‡§®‡•á‡§ü ‡§ß‡•ã‡§ï‡§æ",
        
        # Identity Theft & Fraud
        "‡§Ü‡§á‡§°‡•á‡§®‡•ç‡§ü‡§ø‡§ü‡•Ä ‡§ö‡•ã‡§∞‡•Ä", "‡§™‡§π‡§ø‡§ö‡§æ‡§® ‡§ö‡•ã‡§∞‡•Ä", "‡§´‡§∞‡§ú‡•Ä‡§µ‡•Ä", "‡§®‡§ï‡§≤‡•Ä", "‡§ò‡•ã‡§ü‡§æ‡§≤‡§æ", "‡§ß‡•ã‡§ï‡§æ",
        "‡§´‡§ø‡§®‡§æ‡§®‡•ç‡§∏‡§ø‡§Ø‡§≤ ‡§ß‡•ã‡§ï‡§æ", "‡§á‡§®‡•ç‡§≠‡•á‡§∏‡•ç‡§ü‡§Æ‡•á‡§®‡•ç‡§ü ‡§ß‡•ã‡§ï‡§æ", "‡§™‡•ã‡§®‡•ç‡§ú‡•Ä ‡§Ø‡•ã‡§ú‡§®‡§æ",
        
        # AI & Emerging Tech Threats
        "‡§è‡§Ü‡§à", "‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ", "‡§Æ‡•á‡§∏‡§ø‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§ô", "‡§°‡§ø‡§™ ‡§≤‡§∞‡•ç‡§®‡§ø‡§ô", "‡§®‡•ç‡§Ø‡•Å‡§∞‡§≤ ‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï",
        "‡§è‡§Ü‡§à ‡§π‡§•‡§ø‡§Ø‡§æ‡§∞", "‡§∏‡•ç‡§µ‡§æ‡§Ø‡§§‡•ç‡§§ ‡§π‡§•‡§ø‡§Ø‡§æ‡§∞", "‡§∞‡•ã‡§¨‡•ã‡§ü‡§ø‡§ï‡•ç‡§∏", "‡§°‡•ç‡§∞‡•ã‡§®",
        "‡§∏‡•ç‡§µ‡§ö‡§æ‡§≤‡§ø‡§§ ‡§π‡§Æ‡§≤‡§æ", "‡§è‡§Ü‡§à ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§Æ‡•á‡§∏‡§ø‡§® ‡§á‡§®‡•ç‡§ü‡•á‡§≤‡§ø‡§ú‡•á‡§®‡•ç‡§∏",
        
        # Critical Infrastructure
        "‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§∏‡§ø‡§ü‡•Ä", "‡§á‡§®‡•ç‡§ü‡§∞‡§®‡•á‡§ü ‡§Ö‡§´ ‡§•‡§ø‡§ô‡•ç‡§∏", "‡§Ü‡§á‡§ì‡§ü‡•Ä", "‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§ó‡•ç‡§∞‡§ø‡§°",
        "‡§ä‡§∞‡•ç‡§ú‡§æ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä", "‡§ú‡§≤‡§æ‡§™‡•Ç‡§∞‡•ç‡§§‡§ø", "‡§Ø‡§æ‡§§‡§æ‡§Ø‡§æ‡§§ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä", "‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä",
        "‡§Ü‡§≤‡•ã‡§ö‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ", "‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ",
        
        # Government & Politics
        "‡§ó‡•É‡§π‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä", "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä", "‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø", "‡§∏‡§∞‡§ï‡§æ‡§∞", "‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä", "‡§∏‡§Ç‡§∏‡§¶",
        "‡§ì‡§≤‡•Ä", "‡§™‡•ç‡§∞‡§ö‡§£‡•ç‡§°", "‡§¶‡•á‡§µ", "‡§∏‡•å‡§∞‡•ç‡§Ø", "‡§®‡•á‡§™‡§æ‡§≤‡•Ä", "‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏", "‡§è‡§Æ‡§æ‡§≤‡•á", "‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡•Ä",
        
        # Army & Security Forces
        "‡§∏‡•á‡§®‡§æ", "‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§∏‡•á‡§®‡§æ", "‡§∏‡•à‡§®‡§ø‡§ï", "‡§∏‡§∂‡§∏‡•ç‡§§‡•ç‡§∞", "‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§ú‡•á‡§µ‡•Ä", "‡§ú‡§µ‡§æ‡§®",
        "‡§™‡•ç‡§∞‡§π‡§∞‡•Ä", "‡§®‡•á‡§™‡§æ‡§≤ ‡§™‡•ç‡§∞‡§π‡§∞‡•Ä", "‡§è‡§™‡•Ä‡§è‡§´", "‡§∏‡§∂‡§∏‡•ç‡§§‡•ç‡§∞ ‡§™‡•ç‡§∞‡§π‡§∞‡•Ä",
        
        # Crime & Violence
        "‡§π‡§§‡•ç‡§Ø‡§æ", "‡§°‡§ï‡•à‡§§‡•Ä", "‡§ö‡•ã‡§∞‡•Ä", "‡§≤‡•Å‡§ü", "‡§Ö‡§™‡§π‡§∞‡§£", "‡§¨‡§≤‡§æ‡§§‡•ç‡§ï‡§æ‡§∞", "‡§π‡§ø‡§Ç‡§∏‡§æ",
        "‡§Æ‡§æ‡§∞‡§™‡•Ä‡§ü", "‡§Ü‡§ï‡•ç‡§∞‡§Æ‡§£", "‡§ß‡§Æ‡•ç‡§ï‡•Ä", "‡§Ö‡§™‡§∞‡§æ‡§ß", "‡§Ö‡§™‡§∞‡§æ‡§ß‡•Ä",
        
        # Protests & Civil Unrest
        "‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®", "‡§Ü‡§®‡•ç‡§¶‡•ã‡§≤‡§®", "‡§ß‡§∞‡•ç‡§®‡§æ", "‡§π‡§°‡•ç‡§§‡§æ‡§≤", "‡§≠‡•ã‡§ï‡§π‡§°‡•ç‡§§‡§æ‡§≤", "‡§ú‡•Å‡§≤‡•Å‡§∏", "‡§∞‡•à‡§≤‡•Ä",
        
        # Legal & Court
        "‡§Ö‡§¶‡§æ‡§≤‡§§", "‡§∏‡•Å‡§®‡•Å‡§µ‡§æ‡§á", "‡§®‡•ç‡§Ø‡§æ‡§Ø", "‡§ú‡•á‡§≤", "‡§ï‡§æ‡§∞‡§æ‡§µ‡§æ‡§∏", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ", "‡§Ø‡§æ‡§ö‡§ø‡§ï‡§æ",
        
        # Economic Threats
        "‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§µ‡•É‡§¶‡•ç‡§ß‡§ø", "‡§Æ‡§π‡§Å‡§ó‡•Ä", "‡§Ö‡§µ‡§∞‡•ã‡§ß", "‡§®‡§æ‡§ï‡§æ‡§¨‡§®‡•ç‡§¶‡•Ä", "‡§∏‡§Ç‡§ï‡§ü", "‡§Æ‡§®‡•ç‡§¶‡•Ä", "‡§¨‡•á‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞",
        
        # GenZ & Youth
        "‡§ú‡•á‡§®‡§ú‡•á‡§°", "‡§ú‡•á‡§® ‡§ú‡•á‡§°", "‡§Ø‡•Å‡§µ‡§æ", "‡§Ø‡•Å‡§µ‡§§‡•Ä", "‡§Ø‡•Å‡§µ‡§ï", "‡§õ‡§æ‡§§‡•ç‡§∞", "‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä",
        
        # Durga Parsai
        "‡§¶‡•Å‡§∞‡•ç‡§ó‡§æ ‡§™‡§æ‡§∞‡•ç‡§∏‡§æ‡§à", "‡§™‡§æ‡§∞‡•ç‡§∏‡§æ‡§à", "‡§¶‡•Å‡§∞‡•ç‡§ó‡§æ", "‡§™‡§æ‡§∞‡§∏‡§æ‡§à", "‡§°‡•Ä‡§™‡•Ä",
        
        # Esewa & Digital Payments
        "‡§á‡§∏‡•á‡§µ‡§æ", "‡§à-‡§∏‡•á‡§µ‡§æ", "‡§á‡§∏‡•áwa", "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§≤‡•á‡§®‡§¶‡•á‡§®", "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§µ‡§æ‡§≤‡•ç‡§ü", "‡§∞‡•ã‡§π‡§ø‡§§ ‡§™‡•å‡§°‡•á‡§≤"
    }
    
    # Pre-compile regex patterns
    URL_PATTERN = re.compile(r'/(\d{4})/(\d+)/')
    CONTENT_SELECTORS = [
        "body > main > section.custom-container.mt-10 > div.row > div.col-xl-8 > div > div.col-lg-11 > div > div.content__with-sidebar > div > div.content__desc > div",
        "div.news_detail-para.para.detail-content-paragraph.detail-news-details-paragh",
        "div.content__desc div",
        "div.detail-content-paragraph",
        "div.news_detail-para"
    ]
    
    # Pre-define threat level categories for faster lookup
    CRITICAL_THREAT_TERMS = {
        "‡§π‡•ç‡§Ø‡§æ‡§ï", "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§π‡§Æ‡§≤‡§æ", "‡§°‡§æ‡§ü‡§æ ‡§â‡§≤‡•ç‡§≤‡§Ç‡§ò‡§®", "‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§∏‡§Æ‡§µ‡•á‡§Ø‡§∞", 
        "‡§ï‡•ç‡§∞‡•á‡§°‡§ø‡§ü ‡§ï‡§æ‡§∞‡•ç‡§° ‡§ß‡•ã‡§ï‡§æ", "‡§¨‡•à‡§Ç‡§ï‡§ø‡§ô ‡§ß‡•ã‡§ï‡§æ", "‡§Ü‡§á‡§°‡•á‡§®‡•ç‡§ü‡§ø‡§ü‡•Ä ‡§ö‡•ã‡§∞‡•Ä",
        "‡§π‡§§‡•ç‡§Ø‡§æ", "‡§Ü‡§§‡§Ç‡§ï‡§µ‡§æ‡§¶", "‡§¨‡§Æ", "‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü", "‡§Ö‡§™‡§π‡§∞‡§£", "‡§¨‡§≤‡§æ‡§§‡•ç‡§ï‡§æ‡§∞"
    }
    HIGH_THREAT_TERMS = {
        "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§Ö‡§™‡§∞‡§æ‡§ß", "‡§Æ‡§æ‡§≤‡§µ‡•á‡§Ø‡§∞", "‡§´‡§ø‡§∏‡§ø‡§ô", "‡§°‡§æ‡§ü‡§æ ‡§≤‡§ø‡§ï", 
        "‡§ó‡•ã‡§™‡§®‡•Ä‡§Ø‡§§‡§æ ‡§â‡§≤‡•ç‡§≤‡§Ç‡§ò‡§®", "‡§∏‡•ã‡§∂‡§≤ ‡§Æ‡§ø‡§°‡§ø‡§Ø‡§æ ‡§Ö‡§™‡§∞‡§æ‡§ß",
        "‡§™‡§ï‡•ç‡§∞‡§æ‡§â", "‡§ß‡§∞‡•å‡§ü‡•Ä", "‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®", "‡§π‡§ø‡§Ç‡§∏‡§æ", "‡§ò‡•ã‡§ü‡§æ‡§≤‡§æ", "‡§≠‡•ç‡§∞‡§∑‡•ç‡§ü‡§æ‡§ö‡§æ‡§∞"
    }
    MEDIUM_THREAT_TERMS = {
        "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§°‡§æ‡§ü‡§æ ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§£", "‡§Ö‡§®‡§≤‡§æ‡§á‡§® ‡§ß‡•ã‡§ï‡§æ",
        "‡§è‡§Ü‡§à", "‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ", "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§Ö‡§™‡§∞‡§æ‡§ß",
        "‡§ú‡•á‡§®‡§ú‡•á‡§°", "‡§Ø‡•Å‡§µ‡§æ", "‡§¶‡•Å‡§∞‡•ç‡§ó‡§æ ‡§™‡§æ‡§∞‡•ç‡§∏‡§æ‡§à", "‡§∏‡•á‡§®‡§æ", "‡§™‡•ç‡§∞‡§π‡§∞‡•Ä"
    }
    
    # Create keyword set for faster lookup
    keyword_set = set(important_keywords)
    
    # Rate limiting class
    class RateLimiter:
        def __init__(self, calls_per_minute=20):
            self.calls_per_minute = calls_per_minute
            self.last_calls = []
            
        def wait_if_needed(self):
            now = datetime.now()
            # Remove calls older than 1 minute
            self.last_calls = [call for call in self.last_calls 
                              if (now - call).seconds < 60]
            
            if len(self.last_calls) >= self.calls_per_minute:
                oldest_call = self.last_calls[0]
                sleep_time = 60 - (now - oldest_call).seconds
                if sleep_time > 0:
                    print(f"‚è≥ Rate limiting: Sleeping for {sleep_time} seconds...")
                    time.sleep(sleep_time + random.uniform(1, 3))
                
                # Clean up again after sleep
                now = datetime.now()
                self.last_calls = [call for call in self.last_calls 
                                  if (now - call).seconds < 60]
            
            self.last_calls.append(now)
    
    def save_to_debug_file(data, filename="techpana_debug_output.json"):
        """Save the scraped data to a JSON file for debugging"""
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"üíæ Debug data saved to {filename}")
        except Exception as e:
            print(f"‚ùå Error saving debug file: {e}")
    
    def get_existing_urls_from_database():
        """Get existing article URLs from database to avoid duplicates"""
        try:
            # Import your model here
            from collect.models import AutoNewsArticle
            existing_urls = set(AutoNewsArticle.objects.values_list('url', flat=True))
            print(f"üìÅ Found {len(existing_urls)} existing articles in database")
            return existing_urls
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading existing articles from database: {e}")
            return set()
    
    def is_recent_article_by_id(article_url):
        """Check if article is from today or yesterday based on article ID pattern"""
        try:
            match = URL_PATTERN.search(article_url)
            if not match:
                return False
            
            year = int(match.group(1))
            article_id = int(match.group(2))
            
            current_year = datetime.now().year
            if year != current_year:
                return False
            
            # Adjust this threshold based on typical article frequency
            # Techpana seems to publish articles with IDs around 155xxx in 2026
            current_max_id = 155000  # Adjust this based on current articles
            
            # Consider articles from the last 1000 IDs as recent
            return article_id >= (current_max_id - 1000)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error checking article ID for {article_url}: {e}")
            return False

    def extract_article_info(container):
        """Extract title and link from article container"""
        for heading_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            heading = container.find(heading_tag)
            if heading:
                title_link = heading.find('a', href=URL_PATTERN)
                if title_link:
                    title = title_link.get_text(strip=True)
                    link = title_link.get('href')
                    if link and not link.startswith('http'):
                        link = "https://techpana.com" + link
                    return title, link
        
        # Alternative selector
        title_div = container.find("div", class_=re.compile(r"single_row-title|single_grid-title"))
        if title_div:
            title_link = title_div.find('a', href=URL_PATTERN)
            if title_link:
                title = title_link.get_text(strip=True)
                link = title_link.get('href')
                if link and not link.startswith('http'):
                    link = "https://techpana.com" + link
                return title, link
        
        return None, None

    def create_session():
        """Create a session with retry strategy and rate limiting"""
        session = requests.Session()
        
        # Configure retry strategy
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=10
        )
        
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        
        # Update headers with random User-Agent
        session.headers.update({
            "User-Agent": random.choice(user_agents),
            "Accept": headers["Accept"],
            "Accept-Language": headers["Accept-Language"],
            "Accept-Encoding": headers["Accept-Encoding"],
            "DNT": headers["DNT"],
            "Connection": headers["Connection"],
            "Upgrade-Insecure-Requests": headers["Upgrade-Insecure-Requests"],
        })
        
        return session

    def fetch_single_article_content(url, session, rate_limiter):
        """Fetch content for a single article with rate limiting"""
        try:
            # Apply rate limiting
            rate_limiter.wait_if_needed()
            
            # Add random delay between 3-7 seconds
            time.sleep(random.uniform(3, 7))
            
            response = session.get(url, timeout=15)
            response.raise_for_status()
            
            # Check if we got blocked
            if response.status_code == 429:
                print(f"‚è∏Ô∏è Got 429 for {url}, waiting 30 seconds...")
                time.sleep(30)
                return url, ""
            
            soup = BeautifulSoup(response.content, "lxml")
            full_content = ""
            
            for selector in CONTENT_SELECTORS:
                content_div = soup.select_one(selector)
                if content_div:
                    paragraphs = content_div.find_all("p")
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if (text and len(text) > 15 and
                            not text.startswith(("‡§™‡§õ‡§ø‡§≤‡•ç‡§≤‡•ã ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§µ‡§ß‡§ø‡§ï:", "‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§µ‡§ß‡§ø‡§ï:", "Updated:")) and
                            not any(date_term in text for date_term in ["‡§Æ‡§Ç‡§∏‡§ø‡§∞", "‡§ï‡§æ‡§∞‡•ç‡§§‡§ø‡§ï", "‡•®‡•¶‡•Æ‡•®", "‡•®‡•¶‡•Æ‡•ß"]) and
                            "iframe" not in str(p) and
                            "facebook" not in str(p).lower() and
                            "comment" not in str(p).lower()):
                            
                            full_content += text + " "
                    
                    if len(full_content.strip()) > 100:
                        break
            
            return url, full_content.strip()[:2000] if full_content.strip() else ""
            
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error fetching {url}: {e}")
            return url, ""
        except Exception as e:
            print(f"‚ùå Unexpected error fetching {url}: {e}")
            return url, ""

    def get_article_content_batch(urls_to_fetch):
        """Fetch multiple article contents in parallel with better rate limiting"""
        content_map = {}
        rate_limiter = RateLimiter(calls_per_minute=15)  # Be conservative
        
        # Process in small batches with delays between batches
        batch_size = 3  # Very small batch size to avoid detection
        delay_between_batches = 20  # seconds
        
        for i in range(0, len(urls_to_fetch), batch_size):
            batch = urls_to_fetch[i:i+batch_size]
            print(f"üìÑ Processing batch {i//batch_size + 1}/{(len(urls_to_fetch)-1)//batch_size + 1} ({len(batch)} articles)")
            
            # Create a new session for each batch to rotate User-Agent
            session = create_session()
            
            # Process batch
            with ThreadPoolExecutor(max_workers=2) as executor:  # Only 2 workers!
                future_to_url = {}
                for url in batch:
                    future = executor.submit(fetch_single_article_content, url, session, rate_limiter)
                    future_to_url[future] = url
                
                for future in as_completed(future_to_url):
                    url, content = future.result()
                    content_map[url] = content
            
            # Close session
            session.close()
            
            # Delay between batches
            if i + batch_size < len(urls_to_fetch):
                print(f"‚è∏Ô∏è Waiting {delay_between_batches} seconds before next batch...")
                time.sleep(delay_between_batches)
        
        return content_map

    def analyze_keywords(content):
        """Analyze content for keywords and return matches"""
        # Use list comprehension for faster matching
        found_keywords = [kw for kw in keyword_set if kw in content]
        return found_keywords

    def determine_threat_level(keywords):
        """Determine threat level based on found keywords"""
        if any(term in keywords for term in CRITICAL_THREAT_TERMS):
            return "critical"
        elif any(term in keywords for term in HIGH_THREAT_TERMS):
            return "high"
        elif any(term in keywords for term in MEDIUM_THREAT_TERMS):
            return "medium"
        else:
            return "low"

    def categorize_article(keywords):
        """Categorize article based on keywords"""
        categories = []
        category_mapping = {
            "Cybersecurity": {"‡§∏‡§æ‡§á‡§¨‡§∞", "‡§π‡•ç‡§Ø‡§æ‡§ï", "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§Ö‡§™‡§∞‡§æ‡§ß", "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§π‡§Æ‡§≤‡§æ", "‡§Æ‡§æ‡§≤‡§µ‡•á‡§Ø‡§∞", "‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§∏‡§Æ‡§µ‡•á‡§Ø‡§∞"},
            "Data_Privacy": {"‡§°‡§æ‡§ü‡§æ", "‡§°‡§æ‡§ü‡§æ ‡§â‡§≤‡•ç‡§≤‡§Ç‡§ò‡§®", "‡§ó‡•ã‡§™‡§®‡•Ä‡§Ø‡§§‡§æ", "‡§°‡§æ‡§ü‡§æ ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§£", "‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§°‡§æ‡§ü‡§æ"},
            "Social_Media": {"‡§∏‡•ã‡§∂‡§≤ ‡§Æ‡§ø‡§°‡§ø‡§Ø‡§æ", "‡§´‡•á‡§∏‡§¨‡•Å‡§ï", "‡§ü‡•ç‡§µ‡§ø‡§ü‡§∞", "‡§á‡§®‡•ç‡§∏‡•ç‡§ü‡§æ‡§ó‡•ç‡§∞‡§æ‡§Æ", "‡§ü‡§ø‡§ï‡§ü‡§ï"},
            "AI_Threats": {"‡§è‡§Ü‡§à", "‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ", "‡§Æ‡•á‡§∏‡§ø‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§ô", "‡§°‡§ø‡§™ ‡§≤‡§∞‡•ç‡§®‡§ø‡§ô"},
            "Financial_Tech": {"‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§¨‡•à‡§Ç‡§ï‡§ø‡§ô", "‡§ï‡•ç‡§∞‡•á‡§°‡§ø‡§ü ‡§ï‡§æ‡§∞‡•ç‡§°", "‡§à-‡§ï‡•â‡§Æ‡§∞‡•ç‡§∏ ‡§ß‡•ã‡§ï‡§æ", "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§™‡•á‡§Æ‡•á‡§®‡•ç‡§ü"},
            "GenZ": {"‡§ú‡•á‡§®‡§ú‡•á‡§°", "‡§ú‡•á‡§® ‡§ú‡•á‡§°", "‡§Ø‡•Å‡§µ‡§æ", "‡§Ø‡•Å‡§µ‡§§‡•Ä", "‡§Ø‡•Å‡§µ‡§ï"},
            "Durga_Parsai": {"‡§¶‡•Å‡§∞‡•ç‡§ó‡§æ ‡§™‡§æ‡§∞‡•ç‡§∏‡§æ‡§à", "‡§™‡§æ‡§∞‡•ç‡§∏‡§æ‡§à", "‡§¶‡•Å‡§∞‡•ç‡§ó‡§æ", "‡§™‡§æ‡§∞‡§∏‡§æ‡§à"},
            "Army": {"‡§∏‡•á‡§®‡§æ", "‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§∏‡•á‡§®‡§æ", "‡§∏‡•à‡§®‡§ø‡§ï", "‡§∏‡§∂‡§∏‡•ç‡§§‡•ç‡§∞"},
            "Police": {"‡§™‡•ç‡§∞‡§π‡§∞‡•Ä", "‡§®‡•á‡§™‡§æ‡§≤ ‡§™‡•ç‡§∞‡§π‡§∞‡•Ä", "‡§™‡•ç‡§∞‡§π‡§∞‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä"},
            "Crime": {"‡§π‡§§‡•ç‡§Ø‡§æ", "‡§π‡§ø‡§Ç‡§∏‡§æ", "‡§Æ‡§æ‡§∞‡§™‡•Ä‡§ü", "‡§Ü‡§ï‡•ç‡§∞‡§Æ‡§£"},
            "Protest": {"‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®", "‡§Ü‡§®‡•ç‡§¶‡•ã‡§≤‡§®", "‡§ß‡§∞‡•ç‡§®‡§æ", "‡§π‡§°‡•ç‡§§‡§æ‡§≤"},
            "Government": {"‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä", "‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø", "‡§∏‡§∞‡§ï‡§æ‡§∞", "‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä"}
        }
        
        for cat, terms in category_mapping.items():
            if any(term in keywords for term in terms):
                categories.append(cat)
        
        return categories if categories else ["General"]

    def get_article_date_from_url(article_url):
        """Extract approximate date from article URL for display"""
        try:
            match = URL_PATTERN.search(article_url)
            if match:
                year = match.group(1)
                article_id = match.group(2)
                return f"{year}-{article_id}"
            return "Recent"
        except:
            return "Recent"

    try:
        print(f"üöÄ Starting Techpana scraping at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Load existing articles from database
        existing_urls = get_existing_urls_from_database()
        
        # Create session for homepage
        session = create_session()
        
        # Fetch main page with delay
        time.sleep(random.uniform(2, 5))
        print(f"üåê Fetching homepage: {base_url}")
        response = session.get(base_url, timeout=15)
        response.raise_for_status()
        
        # Check for blocking
        if response.status_code == 429:
            print("‚ùå Homepage blocked with 429. Waiting 60 seconds...")
            time.sleep(60)
            # Try once more
            response = session.get(base_url, timeout=15)
            response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "lxml")
        session.close()
        
        # Find all article containers
        article_containers = []
        main_articles = soup.select("div.single_grid-wrapper")
        side_articles = soup.select("div.single_row-wrapper")
        grid_articles = soup.select("div.grid_section-content .single_grid-wrapper, div.grid_section-content .single_row-wrapper")
        
        article_containers.extend(main_articles)
        article_containers.extend(side_articles)
        article_containers.extend(grid_articles)
        
        print(f"üîç Found {len(article_containers)} article containers on homepage")
        
        # First pass: extract basic article info and filter
        candidate_articles = []
        candidate_urls = []
        
        for container in article_containers:
            title, link = extract_article_info(container)
            if not title or not link:
                continue
            
            # Skip existing articles
            if link in existing_urls:
                continue
            
            # Check if recent
            if not is_recent_article_by_id(link):
                continue
            
            # Extract description and image
            description = ""
            desc_p = container.find('p')
            if desc_p:
                description = desc_p.get_text(strip=True)
            
            image_url = None
            img_tag = container.find('img')
            if img_tag and img_tag.get('src'):
                image_url = img_tag.get('src')
                if image_url and not image_url.startswith('http'):
                    image_url = "https://techpana.com" + image_url
            
            candidate_articles.append({
                'title': title,
                'link': link,
                'description': description,
                'image_url': image_url
            })
            candidate_urls.append(link)
        
        print(f"üìÑ Found {len(candidate_articles)} candidate articles (after filtering)")
        
        # If too many articles, prioritize by URL pattern or title keywords
        if len(candidate_articles) > 50:
            print(f"‚ö†Ô∏è Too many candidate articles ({len(candidate_articles)}). Prioritizing...")
            # Prioritize articles with higher IDs (more recent)
            candidate_articles.sort(key=lambda x: int(URL_PATTERN.search(x['link']).group(2)) if URL_PATTERN.search(x['link']) else 0, reverse=True)
            candidate_articles = candidate_articles[:50]  # Limit to 50
            candidate_urls = [a['link'] for a in candidate_articles]
        
        if not candidate_articles:
            print("üì≠ No new articles to fetch")
            output = {
                "metadata": {
                    "source": "Techpana",
                    "url": base_url,
                    "scraped_at": datetime.now().isoformat(),
                    "status": "success",
                    "message": "No new articles found",
                    "total_articles_found": len(article_containers)
                },
                "articles": []
            }
            return json.dumps(output, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Fetching content for {len(candidate_articles)} candidate articles...")
        
        # Batch fetch article contents with rate limiting
        content_map = get_article_content_batch(candidate_urls)
        
        # Process articles
        articles_data = []
        stats = {
            'new_articles': 0,
            'skipped_existing': len(article_containers) - len(candidate_articles),
            'skipped_no_content': 0,
            'skipped_no_keywords': 0,
            'skipped_low_priority': 0
        }
        
        for article_info in candidate_articles:
            title = article_info['title']
            link = article_info['link']
            description = article_info['description']
            image_url = article_info['image_url']
            
            print(f"üîÑ Processing: {title[:60]}...")
            
            # Get content
            full_content = content_map.get(link, "")
            summary = full_content if full_content else description
            
            if not summary or len(summary.strip()) < 50:
                stats['skipped_no_content'] += 1
                print(f"  ‚è≠Ô∏è Skipped: No content")
                continue
            
            # Analyze keywords
            content_for_analysis = title + " " + summary
            found_keywords = analyze_keywords(content_for_analysis)
            
            if not found_keywords:
                stats['skipped_no_keywords'] += 1
                print(f"  ‚è≠Ô∏è Skipped: No keywords matched")
                continue
            
            # Determine threat level and priority
            threat_level = determine_threat_level(found_keywords)
            if threat_level == "low":
                stats['skipped_low_priority'] += 1
                print(f"  ‚è≠Ô∏è Skipped: Low priority")
                continue
            
            priority = "high" if threat_level in ["critical", "high"] else "medium"
            categories = categorize_article(found_keywords)
            
            # Create article data
            date_str = get_article_date_from_url(link)
            current_date = datetime.now().strftime("%Y-%m-%d")
            
            article_data = {
                "id": len(articles_data) + 1,
                "title": title,
                "summary": summary,
                "url": link,
                "image_url": image_url if image_url else "",
                "date": f"{current_date} (ID: {date_str})",
                "source": "techpana",
                "threat_analysis": {
                    "level": threat_level,
                    "keywords_found": found_keywords,
                    "total_keywords_matched": len(found_keywords),
                    "categories": categories
                },
                "content_length": len(content_for_analysis),
                "summary_length": len(summary),
                "has_content": bool(summary and len(summary.strip()) >= 50),
                "content_source": "full_article" if full_content else "preview",
                "priority": priority,
                "has_full_content": bool(full_content),
                "scraped_timestamp": datetime.now().isoformat()
            }
            
            articles_data.append(article_data)
            stats['new_articles'] += 1
            print(f"  ‚úÖ Added {priority} priority article with {len(found_keywords)} keywords")
        
        # Final statistics
        print(f"\nüìä SCRAPING SUMMARY:")
        print(f"   Total articles found: {len(article_containers)}")
        print(f"   High/Medium priority articles added: {stats['new_articles']}")
        print(f"   Articles skipped (existing): {stats['skipped_existing']}")
        print(f"   Articles skipped (no content): {stats['skipped_no_content']}")
        print(f"   Articles skipped (no keywords): {stats['skipped_no_keywords']}")
        print(f"   Articles skipped (low priority): {stats['skipped_low_priority']}")
        
        # Create output
        output = {
            "metadata": {
                "source": "Techpana",
                "url": base_url,
                "scraped_at": datetime.now().isoformat(),
                "status": "success",
                "total_articles_found": len(article_containers),
                "high_medium_priority_articles_added": stats['new_articles'],
                "articles_skipped_existing": stats['skipped_existing'],
                "articles_skipped_no_content": stats['skipped_no_content'],
                "articles_skipped_no_keywords": stats['skipped_no_keywords'],
                "articles_skipped_low_priority": stats['skipped_low_priority'],
                "priority_filter": "high_and_medium_only",
                "date_filter": "today_and_yesterday",
                "content_statistics": {
                    "articles_with_full_content": len([a for a in articles_data if a["has_full_content"]]),
                    "articles_with_preview_only": len([a for a in articles_data if not a["has_full_content"]]),
                    "average_summary_length": sum([a["summary_length"] for a in articles_data]) // len(articles_data) if articles_data else 0,
                    "articles_with_adequate_content": len([a for a in articles_data if a["has_content"]])
                },
                "message": f"Found {stats['new_articles']} high/medium priority articles (today/yesterday) from Techpana"
            },
            "articles": articles_data
        }
        
        # Save debug file
        save_to_debug_file(output, "techpana_debug_output.json")
        print(f"üíæ Full results saved to techpana_debug_output.json")
        
        return json.dumps(output, indent=2, ensure_ascii=False)
        
    except Exception as e:
        print(f"‚ùå Critical error: {e}")
        import traceback
        traceback.print_exc()
        
        error_output = {
            "metadata": {
                "source": "Techpana", 
                "url": base_url,
                "scraped_at": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            },
            "articles": []
        }
        save_to_debug_file(error_output, "techpana_debug_output.json")
        return json.dumps(error_output, indent=2)

# Run it
if __name__ == "__main__":
    print("üöÄ Starting Techpana scraper...")
    start_time = time.time()
    json_data = techpana_to_json()
    end_time = time.time()
    print(f"‚úÖ Techpana scraping completed in {end_time - start_time:.2f} seconds!")
    print(f"üìÑ Output length: {len(json_data)} characters")